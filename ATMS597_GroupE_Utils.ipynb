{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATMS597_GroupE_Utils",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bieri2/ATMS-597-Project-4-Wx-Prediction/blob/master/ATMS597_GroupE_Utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVOQmMcZErKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import necessary modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime as dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrHmR-yVh1TB",
        "colab_type": "text"
      },
      "source": [
        "## KCMI Observations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFdHZyGfQGw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in daily KCMI data \n",
        "kcmi_daily = pd.read_csv('/content/drive/My Drive/project4_data/KCMI_daily.csv', \n",
        "                         header = 5, names = ['Timestamp', 'TMAXOBS', 'TMINOBS', \n",
        "                                              'WMAXOBS', 'RTOTOBS', 'extra'],\n",
        "                         dtype = {'TMAXOBS': np.float64, \n",
        "                                  'TMINOBS': np.float64, \n",
        "                                  'WMAXOBS': np.float64, \n",
        "                                  'RTOTOBS': np.float64}, \n",
        "                         na_values = 'M')[:-7]\n",
        "# Drop unnecessary column\n",
        "kcmi_daily = kcmi_daily.drop(columns = 'extra')\n",
        "# Set 'Date' column as index\n",
        "kcmi_daily = kcmi_daily.set_index(pd.to_datetime(kcmi_daily['Timestamp'])).drop(columns = 'Timestamp')\n",
        "# Resample to fill in missing days with NaNs\n",
        "kcmi_daily = kcmi_daily.resample('D').mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7cGmFR1QJ_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in hourly KCMI data\n",
        "kcmi_hourly = pd.read_csv('/content/drive/My Drive/project4_data/KCMI_hourly.csv', \n",
        "                          header = 0, names = ['Timestamp', 'TMPC', 'DWPC', 'PRES', \n",
        "                                               'WDIR', 'WSPD', 'SKCT', 'PRCP1', \n",
        "                                               'PRCP6'], \n",
        "                          dtype = {'TMPC' : np.float64, \n",
        "                                   'DWPC' : np.float64, \n",
        "                                   'PRES' : np.float64, \n",
        "                                   'WDIR' : np.float64,\n",
        "                                   'WSPD' : np.float64, \n",
        "                                   'SKCT' : np.float64, \n",
        "                                   'PRCP1': np.float64, \n",
        "                                   'PRCP6': np.float64}, \n",
        "                          usecols = [0, 1, 2, 3, 4, 5, 6, 7, 8], na_values = 'M')\n",
        "# Set 'Timestep' column as index\n",
        "kcmi_hourly = kcmi_hourly.set_index(pd.to_datetime(kcmi_hourly['Timestamp'])).drop(columns = 'Timestamp')\n",
        "# Consider only data for 2010 to 2019\n",
        "kcmi_hourly = kcmi_hourly['2010-01-01':'2019-12-31']\n",
        "# Set trace precipitation to NaN\n",
        "kcmi_hourly['PRCP1'][kcmi_hourly['PRCP1'] < 0] = np.NaN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcwAtsSrQMuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Resample hourly data to daily by summing hourly values\n",
        "kcmi_hourly_resampled = kcmi_hourly.resample('D').apply(lambda x: x.values.sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Baj2xMWEQNMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace daily precip data with aggregated hourly data, since daily precip data is problematic\n",
        "kcmi_daily['RTOTOBS'] = kcmi_hourly_resampled['PRCP1'].astype('float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkeIYFucQP8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert observations to metric units if necessary\n",
        "kcmi_daily['TMAXOBS'] = (kcmi_daily['TMAXOBS'] - 32.)/1.8 # F to C\n",
        "kcmi_daily['TMINOBS'] = (kcmi_daily['TMINOBS'] - 32.)/1.8 # F to C\n",
        "kcmi_daily['WMAXOBS'] = kcmi_daily['WMAXOBS']/2.237 # mph to m/s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrf3YzIGh6Q7",
        "colab_type": "text"
      },
      "source": [
        "## GFS Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFpGy55ET1GW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to decompress tar.gz files and add to directory\n",
        "# Only need to do this once, afterwards just need to read from Drive\n",
        "# Change paths/filenames as needed\n",
        "\n",
        "# ! mkdir '/content/drive/My Drive/project4_data/sfc_tar'\n",
        "# ! gunzip '/content/drive/My Drive/project4_data/sfc.tar.gz' \n",
        "# ! tar -xvf '/content/drive/My Drive/project4_data/sfc.tar' --directory '/content/drive/My Drive/project4_data/sfc_tar'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh1H8FVWVtww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Paths to files for different GFS datasets\n",
        "# Change as needed\n",
        "gfs_daily_dir = '/content/drive/My Drive/project4_data/daily_tar/bufkit/' # Daily GFS data\n",
        "gfs_prof_dir  = '/content/drive/My Drive/project4_data/prof_tar/bufkit/'  # GFS 3-hr vertical profile \n",
        "gfs_sfc_dir   = '/content/drive/My Drive/project4_data/sfc_tar/bufkit/'   # GFS 3-hr surface data "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed1hBCKSj0d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define function to read in GFS daily data one day at a time\n",
        "# Once all files have been read, combine into one DataFrame \n",
        "def get_gfs_data(gfs_dir, prof = False, sfc = False):\n",
        "\n",
        "  # Get list of files to be read\n",
        "  files = os.listdir(gfs_dir)\n",
        "  # Sort so that files are in order by date\n",
        "  files.sort()\n",
        "  # Create list of full paths for all files\n",
        "  file_list = [gfs_dir + f for f in files]\n",
        "\n",
        "  # Create empty lists to hold data\n",
        "  if prof:\n",
        "      all_dwpc = []\n",
        "      all_hght = []\n",
        "      all_tmpc = []\n",
        "      all_uwnd = []\n",
        "      all_vwnd = [] \n",
        "  else:\n",
        "      all_dfs  = []\n",
        "\n",
        "\n",
        "  # Read in each file and add to list\n",
        "  for f in file_list: \n",
        "      # Tell user which file is being read\n",
        "      print('Reading ' + f)\n",
        "\n",
        "      # Do this if reading profile data\n",
        "      if prof:\n",
        "          # Read file\n",
        "          current = pd.read_csv(f, index_col = 0, header = 0,\n",
        "                                names = ['Timestamp', 'DWPC', 'HGHT', \n",
        "                                         'PRES', 'TMPC', 'UWND', 'VWND'])  \n",
        "          \n",
        "          # Loop through values for each time step\n",
        "          # Read in each variable in a separate DataFrame\n",
        "          for i in current.index:\n",
        "              dwpc_current = pd.DataFrame(current['DWPC'][i][1:-1].split(',')).T\n",
        "              dwpc_current['Timestamp'] = [dt.datetime.strptime(i, '%Y-%m-%d %H:%M:%S')]\n",
        "              dwpc_current = dwpc_current.set_index('Timestamp')\n",
        "              dwpc_current = dwpc_current.rename(columns = {0:'DWPC925', 1:'DWPC850', 2:'DWPC700', \n",
        "                                                            3:'DWPC500', 4:'DWPC250', 5:'DWPC100'})\n",
        "              all_dwpc.append(dwpc_current) \n",
        "\n",
        "              hght_current = pd.DataFrame(current['HGHT'][i][1:-1].split(',')).T \n",
        "              hght_current['Timestamp'] = [dt.datetime.strptime(i, '%Y-%m-%d %H:%M:%S')]\n",
        "              hght_current = hght_current.set_index('Timestamp')\n",
        "              hght_current = hght_current.rename(columns = {0:'HGHT925', 1:'HGHT850', 2:'HGHT700', \n",
        "                                                            3:'HGHT500', 4:'HGHT250', 5:'HGHT100'})\n",
        "              all_hght.append(hght_current) \n",
        "\n",
        "              tmpc_current = pd.DataFrame(current['TMPC'][i][1:-1].split(',')).T \n",
        "              tmpc_current['Timestamp'] = [dt.datetime.strptime(i, '%Y-%m-%d %H:%M:%S')]\n",
        "              tmpc_current = tmpc_current.set_index('Timestamp')\n",
        "              tmpc_current = tmpc_current.rename(columns = {0:'TMPC925', 1:'TMPC850', 2:'TMPC700', \n",
        "                                                            3:'TMPC500', 4:'TMPC250', 5:'TMPC100'})\n",
        "              all_tmpc.append(tmpc_current) \n",
        "\n",
        "              uwnd_current = pd.DataFrame(current['UWND'][i][1:-1].split(',')).T \n",
        "              uwnd_current['Timestamp'] = [dt.datetime.strptime(i, '%Y-%m-%d %H:%M:%S')]\n",
        "              uwnd_current = uwnd_current.set_index('Timestamp')\n",
        "              uwnd_current = uwnd_current.rename(columns = {0:'UWND925', 1:'UWND850', 2:'UWND700', \n",
        "                                                            3:'UWND500', 4:'UWND250', 5:'UWND100'})\n",
        "              all_uwnd.append(uwnd_current) \n",
        "\n",
        "              vwnd_current = pd.DataFrame(current['VWND'][i][1:-1].split(',')).T \n",
        "              vwnd_current['Timestamp'] = [dt.datetime.strptime(i, '%Y-%m-%d %H:%M:%S')]\n",
        "              vwnd_current = vwnd_current.set_index('Timestamp')\n",
        "              vwnd_current = vwnd_current.rename(columns = {0:'VWND925', 1:'VWND850', 2:'VWND700', \n",
        "                                                            3:'VWND500', 4:'VWND250', 5:'VWND100'})\n",
        "              all_vwnd.append(vwnd_current) \n",
        "      # Do this if reading surface data\n",
        "      elif sfc:\n",
        "          current = pd.read_csv(f).T.reset_index()[1:].rename(columns = {'index': 'Timestamp', 0: 'DWPC', \n",
        "                                                                          1: 'HCLD', 2: 'LCLD', 3: 'MCLD', \n",
        "                                                                          4: 'PRCP', 5: 'PRES', 6: 'TMPC', \n",
        "                                                                          7: 'UWND', 8: 'VWND', 9: 'WSPD'})\n",
        "          current = current.set_index('Timestamp')\n",
        "\n",
        "          all_dfs.append(current)\n",
        "      # Do this if reading daily data\n",
        "      else:\n",
        "          current = pd.read_csv(f)\n",
        "          current = current.rename(columns = {'Unnamed: 0': 'Timestamp', \n",
        "                                                    'TMAX': 'TMAXGFS',\n",
        "                                                    'TMIN': 'TMINGFS',\n",
        "                                                    'WMAX': 'WMAXGFS',\n",
        "                                                    'RTOT': 'RTOTGFS'})\n",
        "          \n",
        "          current = current.set_index(pd.to_datetime(current['Timestamp'])).drop(columns = 'Timestamp')\n",
        "\n",
        "          all_dfs.append(current) \n",
        "\n",
        "  if prof:\n",
        "      # Concatenate DataFrames\n",
        "      all_dwpc = pd.concat(all_dwpc).astype(np.float64)\n",
        "      all_hght = pd.concat(all_hght).astype(np.float64)\n",
        "      all_tmpc = pd.concat(all_tmpc).astype(np.float64)\n",
        "      all_uwnd = pd.concat(all_uwnd).astype(np.float64)\n",
        "      all_vwnd = pd.concat(all_vwnd).astype(np.float64)\n",
        "\n",
        "      all_prof = pd.concat([all_dwpc, all_hght, all_tmpc, \n",
        "                            all_uwnd, all_vwnd], axis = 1)\n",
        "      \n",
        "      return all_prof\n",
        "\n",
        "  else: \n",
        "      # Create one DataFrame with all data \n",
        "      all_dfs = pd.concat(all_dfs).astype(np.float64)\n",
        "\n",
        "      if not sfc:\n",
        "          # Resample to fill any missing days\n",
        "          all_dfs = all_dfs.resample('D').mean()\n",
        "\n",
        "      return all_dfs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROpkkx1vrPQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in sfc GFS data\n",
        "sfc = get_gfs_data(gfs_sfc_dir, sfc = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0jhN-3qHmyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in profile GFS data\n",
        "prof = get_gfs_data(gfs_prof_dir, prof = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLzS7rwEjLCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Resample to add missing time steps\n",
        "sfc  = sfc.set_index(pd.to_datetime(sfc.index)).resample('3H').mean()\n",
        "prof = prof.resample('3H').mean()\n",
        "# Put 3-hourly data in one DataFrame\n",
        "all_3hr_data = pd.concat([sfc['2010-01-01':'2019-12-31'], prof['2010-01-01':'2019-12-31']], axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on4qYBkQatgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mask out bad data in cloud variables\n",
        "all_3hr_data['LCLD'][all_3hr_data['LCLD'] < 0] = np.NaN\n",
        "all_3hr_data['MCLD'][all_3hr_data['MCLD'] < 0] = np.NaN\n",
        "all_3hr_data['HCLD'][all_3hr_data['HCLD'] < 0] = np.NaN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMB4eaZbZhQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write to csv\n",
        "all_3hr_data.to_csv('/content/drive/My Drive/ATMS 597 Project 4/all_data_3hr.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xu-waOe2x7_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in daily GFS data\n",
        "gfs_daily = get_gfs_data(gfs_daily_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvz7bvWO3FrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Increase all dates in GFS data indices by one day since GFS forecasts apply to the following day\n",
        "gfs_daily = gfs_daily.set_index(gfs_daily.index + pd.Timedelta(days = 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-LqK4-Vd2wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put obs and model data in one DataFrame\n",
        "all_daily_data = pd.concat([kcmi_daily['2010-01-02':], gfs_daily[:'2019-12-31']], axis = 1)\n",
        "# Write to csv\n",
        "all_daily_data.to_csv('/content/drive/My Drive/ATMS 597 Project 4/all_data_daily.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHZ6QgFIuAaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}